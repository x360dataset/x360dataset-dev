# Video Self-supervised Methods

This repo is mainly based on [ActionFormer](https://github.com/happyharrycn/actionformer_release): Localizing Moments of Actions with Transformers

<div align="center">
  <img src="teaser.jpg" width="600px"/>
</div>

## Main Dependencies

+ Ubuntu 16.04
+ CUDA Version: 11.1
+ PyTorch 1.8.1
+ torchvision 0.9.1
+ python 3.7.6

### Data Preparation

Download the Original [Dataset](https://x360dataset.github.io/).

## Installation

* Follow INSTALL.md for installing necessary dependencies and compiling the code.

### Train and Test

You can train the actionformer with multi-modality and test it by simply running

```bash run.sh ```



#### Acknowledgement

If you use this repo, please cite the original work [ActionFormer](https://github.com/happyharrycn/actionformer_release)
